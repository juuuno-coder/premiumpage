
import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import re

# ì„¤ì •
TARGET_SITES = [
    {"name": "hs-tech", "url": "https://www.hs-tech.co.kr/"},
    {"name": "hangseong", "url": "http://www.hangseong.co.kr/"},
    {"name": "gentop", "url": "http://www.gentop.co.kr/"}
]
BASE_DIR = "public/downloads"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def clean_filename(url):
    """URLì—ì„œ íŒŒì¼ëª…ì„ ì¶”ì¶œí•˜ê³  ì•ˆì „í•œ ì´ë¦„ìœ¼ë¡œ ë³€í™˜"""
    path = urlparse(url).path
    name = os.path.basename(path)
    if not name or '.' not in name:
        name = f"image_{int(time.time()*1000)}.jpg"
    name = re.sub(r'[^\w\-.]', '_', name) # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    return name

def download_image(img_url, save_dir):
    """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
    try:
        if img_url.startswith('data:'): return # data URI ì œì™¸
        
        filename = clean_filename(img_url)
        save_path = os.path.join(save_dir, filename)
        
        if os.path.exists(save_path): return # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ

        response = requests.get(img_url, headers=HEADERS, timeout=10, verify=False)
        if response.status_code == 200:
            with open(save_path, 'wb') as f:
                f.write(response.content)
            print(f"  [IMG] Saved: {filename}")
        else:
            print(f"  [ERR] Image download failed: {response.status_code}")
    except Exception as e:
        print(f"  [ERR] Image Error: {e}")

def crawl_site(name, start_url):
    print(f"\nğŸš€ Starting Crawl: {name} ({start_url})")
    
    save_dir = os.path.join(BASE_DIR, name)
    img_dir = os.path.join(save_dir, "images")
    os.makedirs(img_dir, exist_ok=True)
    
    text_file = os.path.join(save_dir, "content.txt")
    
    visited = set()
    to_visit = [start_url]
    domain = urlparse(start_url).netloc
    
    # í…ìŠ¤íŠ¸ íŒŒì¼ ì´ˆê¸°í™”
    with open(text_file, 'w', encoding='utf-8') as f:
        f.write(f"=== {name} Crawled Data ===\n\n")

    count = 0
    max_pages = 20 # ë„ˆë¬´ ë§ì´ ê¸ì§€ ì•Šë„ë¡ ì œí•œ

    while to_visit and count < max_pages:
        url = to_visit.pop(0)
        if url in visited: continue
        visited.add(url)
        count += 1
        
        print(f"Processing ({count}/{max_pages}): {url}")
        
        try:
            resp = requests.get(url, headers=HEADERS, timeout=10, verify=False)
            # ì¸ì½”ë”© ìë™ ê°ì§€ (ì˜¤ë˜ëœ ì‚¬ì´íŠ¸ ëŒ€ë¹„)
            resp.encoding = resp.apparent_encoding 
            
            if resp.status_code != 200: continue
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì €ì¥
            title = soup.title.string.strip() if soup.title else "No Title"
            body_text = soup.body.get_text(separator='\n', strip=True) if soup.body else ""
            cleaned_text = '\n'.join([line for line in body_text.splitlines() if len(line) > 50]) # ë„ˆë¬´ ì§§ì€ ì¤„ ì œì™¸ (ë©”ë‰´ ë“±)
            
            with open(text_file, 'a', encoding='utf-8') as f:
                f.write(f"\n\n--- Page: {title} ({url}) ---\n")
                f.write(cleaned_text[:2000]) # í˜ì´ì§€ë‹¹ í…ìŠ¤íŠ¸ ì–‘ ì œí•œ
                f.write("\n")

            # 2. ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (ë¡œê³ , ì œí’ˆ ì´ë¯¸ì§€ ìœ„ì£¼)
            for img in soup.find_all('img'):
                src = img.get('src')
                if src:
                    full_img_url = urljoin(url, src)
                    download_image(full_img_url, img_dir)
            
            # 3. ë‚´ë¶€ ë§í¬ ìˆ˜ì§‘ (ê°™ì€ ë„ë©”ì¸ë§Œ)
            for a in soup.find_all('a', href=True):
                href = a['href']
                full_url = urljoin(url, href)
                parsed_link = urlparse(full_url)
                
                # ê°™ì€ ë„ë©”ì´ê³ , html/php/jsp ë“±ìœ¼ë¡œ ëë‚˜ëŠ” í…ìŠ¤íŠ¸ í˜ì´ì§€, ì´ë¯¸ ë°©ë¬¸ ì•ˆí•¨
                if parsed_link.netloc == domain and full_url not in visited and full_url not in to_visit:
                    # íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì œì™¸
                    if not any(full_url.endswith(ext) for ext in ['.pdf', '.zip', '.hwp', '.doc']):
                        to_visit.append(full_url)
                        
        except Exception as e:
            print(f"  [ERR] Page Error: {e}")

    print(f"âœ… Finished {name}")

if __name__ == "__main__":
    # SSL ê²½ê³  ë¬´ì‹œ
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    for site in TARGET_SITES:
        crawl_site(site['name'], site['url'])
